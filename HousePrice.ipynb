{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X_train = train_df.drop('SalePrice', axis=1)\n",
    "y_train = train_df['SalePrice']\n",
    "X_test = test_df\n",
    "\n",
    "# List of numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Preprocessing for numerical data: impute missing values with median and scale them\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data: impute missing values with the most frequent value and apply one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a preprocessing and modeling pipeline\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Preprocess the training data\n",
    "X_train_processed = model_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_processed = model_pipeline.transform(X_test)\n",
    "\n",
    "# Now, X_train_processed and X_test_processed are ready for model training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['MSZoning_C (all)' 'MSZoning_FV' 'MSZoning_RH' 'MSZoning_RL'\n",
      " 'MSZoning_RM' 'Street_Grvl' 'Street_Pave' 'Alley_Grvl' 'Alley_Pave'\n",
      " 'Alley_nan' 'LotShape_IR1' 'LotShape_IR2' 'LotShape_IR3' 'LotShape_Reg'\n",
      " 'LandContour_Bnk' 'LandContour_HLS' 'LandContour_Low' 'LandContour_Lvl'\n",
      " 'Utilities_AllPub' 'Utilities_NoSeWa' 'LotConfig_Corner'\n",
      " 'LotConfig_CulDSac' 'LotConfig_FR2' 'LotConfig_FR3' 'LotConfig_Inside'\n",
      " 'LandSlope_Gtl' 'LandSlope_Mod' 'LandSlope_Sev' 'Neighborhood_Blmngtn'\n",
      " 'Neighborhood_Blueste' 'Neighborhood_BrDale' 'Neighborhood_BrkSide'\n",
      " 'Neighborhood_ClearCr' 'Neighborhood_CollgCr' 'Neighborhood_Crawfor'\n",
      " 'Neighborhood_Edwards' 'Neighborhood_Gilbert' 'Neighborhood_IDOTRR'\n",
      " 'Neighborhood_MeadowV' 'Neighborhood_Mitchel' 'Neighborhood_NAmes'\n",
      " 'Neighborhood_NPkVill' 'Neighborhood_NWAmes' 'Neighborhood_NoRidge'\n",
      " 'Neighborhood_NridgHt' 'Neighborhood_OldTown' 'Neighborhood_SWISU'\n",
      " 'Neighborhood_Sawyer' 'Neighborhood_SawyerW' 'Neighborhood_Somerst'\n",
      " 'Neighborhood_StoneBr' 'Neighborhood_Timber' 'Neighborhood_Veenker'\n",
      " 'Condition1_Artery' 'Condition1_Feedr' 'Condition1_Norm'\n",
      " 'Condition1_PosA' 'Condition1_PosN' 'Condition1_RRAe' 'Condition1_RRAn'\n",
      " 'Condition1_RRNe' 'Condition1_RRNn' 'Condition2_Artery'\n",
      " 'Condition2_Feedr' 'Condition2_Norm' 'Condition2_PosA' 'Condition2_PosN'\n",
      " 'Condition2_RRAe' 'Condition2_RRAn' 'Condition2_RRNn' 'BldgType_1Fam'\n",
      " 'BldgType_2fmCon' 'BldgType_Duplex' 'BldgType_Twnhs' 'BldgType_TwnhsE'\n",
      " 'HouseStyle_1.5Fin' 'HouseStyle_1.5Unf' 'HouseStyle_1Story'\n",
      " 'HouseStyle_2.5Fin' 'HouseStyle_2.5Unf' 'HouseStyle_2Story'\n",
      " 'HouseStyle_SFoyer' 'HouseStyle_SLvl' 'RoofStyle_Flat' 'RoofStyle_Gable'\n",
      " 'RoofStyle_Gambrel' 'RoofStyle_Hip' 'RoofStyle_Mansard' 'RoofStyle_Shed'\n",
      " 'RoofMatl_ClyTile' 'RoofMatl_CompShg' 'RoofMatl_Membran' 'RoofMatl_Metal'\n",
      " 'RoofMatl_Roll' 'RoofMatl_Tar&Grv' 'RoofMatl_WdShake' 'RoofMatl_WdShngl'\n",
      " 'Exterior1st_AsbShng' 'Exterior1st_AsphShn' 'Exterior1st_BrkComm'\n",
      " 'Exterior1st_BrkFace' 'Exterior1st_CBlock' 'Exterior1st_CemntBd'\n",
      " 'Exterior1st_HdBoard' 'Exterior1st_ImStucc' 'Exterior1st_MetalSd'\n",
      " 'Exterior1st_Plywood' 'Exterior1st_Stone' 'Exterior1st_Stucco'\n",
      " 'Exterior1st_VinylSd' 'Exterior1st_Wd Sdng' 'Exterior1st_WdShing'\n",
      " 'Exterior2nd_AsbShng' 'Exterior2nd_AsphShn' 'Exterior2nd_Brk Cmn'\n",
      " 'Exterior2nd_BrkFace' 'Exterior2nd_CBlock' 'Exterior2nd_CmentBd'\n",
      " 'Exterior2nd_HdBoard' 'Exterior2nd_ImStucc' 'Exterior2nd_MetalSd'\n",
      " 'Exterior2nd_Other' 'Exterior2nd_Plywood' 'Exterior2nd_Stone'\n",
      " 'Exterior2nd_Stucco' 'Exterior2nd_VinylSd' 'Exterior2nd_Wd Sdng'\n",
      " 'Exterior2nd_Wd Shng' 'MasVnrType_BrkCmn' 'MasVnrType_BrkFace'\n",
      " 'MasVnrType_Stone' 'MasVnrType_nan' 'ExterQual_Ex' 'ExterQual_Fa'\n",
      " 'ExterQual_Gd' 'ExterQual_TA' 'ExterCond_Ex' 'ExterCond_Fa'\n",
      " 'ExterCond_Gd' 'ExterCond_Po' 'ExterCond_TA' 'Foundation_BrkTil'\n",
      " 'Foundation_CBlock' 'Foundation_PConc' 'Foundation_Slab'\n",
      " 'Foundation_Stone' 'Foundation_Wood' 'BsmtQual_Ex' 'BsmtQual_Fa'\n",
      " 'BsmtQual_Gd' 'BsmtQual_TA' 'BsmtQual_nan' 'BsmtCond_Fa' 'BsmtCond_Gd'\n",
      " 'BsmtCond_Po' 'BsmtCond_TA' 'BsmtCond_nan' 'BsmtExposure_Av'\n",
      " 'BsmtExposure_Gd' 'BsmtExposure_Mn' 'BsmtExposure_No' 'BsmtExposure_nan'\n",
      " 'BsmtFinType1_ALQ' 'BsmtFinType1_BLQ' 'BsmtFinType1_GLQ'\n",
      " 'BsmtFinType1_LwQ' 'BsmtFinType1_Rec' 'BsmtFinType1_Unf'\n",
      " 'BsmtFinType1_nan' 'BsmtFinType2_ALQ' 'BsmtFinType2_BLQ'\n",
      " 'BsmtFinType2_GLQ' 'BsmtFinType2_LwQ' 'BsmtFinType2_Rec'\n",
      " 'BsmtFinType2_Unf' 'BsmtFinType2_nan' 'Heating_Floor' 'Heating_GasA'\n",
      " 'Heating_GasW' 'Heating_Grav' 'Heating_OthW' 'Heating_Wall'\n",
      " 'HeatingQC_Ex' 'HeatingQC_Fa' 'HeatingQC_Gd' 'HeatingQC_Po'\n",
      " 'HeatingQC_TA' 'CentralAir_N' 'CentralAir_Y' 'Electrical_FuseA'\n",
      " 'Electrical_FuseF' 'Electrical_FuseP' 'Electrical_Mix' 'Electrical_SBrkr'\n",
      " 'Electrical_nan' 'KitchenQual_Ex' 'KitchenQual_Fa' 'KitchenQual_Gd'\n",
      " 'KitchenQual_TA' 'Functional_Maj1' 'Functional_Maj2' 'Functional_Min1'\n",
      " 'Functional_Min2' 'Functional_Mod' 'Functional_Sev' 'Functional_Typ'\n",
      " 'FireplaceQu_Ex' 'FireplaceQu_Fa' 'FireplaceQu_Gd' 'FireplaceQu_Po'\n",
      " 'FireplaceQu_TA' 'FireplaceQu_nan' 'GarageType_2Types'\n",
      " 'GarageType_Attchd' 'GarageType_Basment' 'GarageType_BuiltIn'\n",
      " 'GarageType_CarPort' 'GarageType_Detchd' 'GarageType_nan'\n",
      " 'GarageFinish_Fin' 'GarageFinish_RFn' 'GarageFinish_Unf'\n",
      " 'GarageFinish_nan' 'GarageQual_Ex' 'GarageQual_Fa' 'GarageQual_Gd'\n",
      " 'GarageQual_Po' 'GarageQual_TA' 'GarageQual_nan' 'GarageCond_Ex'\n",
      " 'GarageCond_Fa' 'GarageCond_Gd' 'GarageCond_Po' 'GarageCond_TA'\n",
      " 'GarageCond_nan' 'PavedDrive_N' 'PavedDrive_P' 'PavedDrive_Y' 'PoolQC_Ex'\n",
      " 'PoolQC_Fa' 'PoolQC_Gd' 'PoolQC_nan' 'Fence_GdPrv' 'Fence_GdWo'\n",
      " 'Fence_MnPrv' 'Fence_MnWw' 'Fence_nan' 'MiscFeature_Gar2'\n",
      " 'MiscFeature_Othr' 'MiscFeature_Shed' 'MiscFeature_TenC'\n",
      " 'MiscFeature_nan' 'SaleType_COD' 'SaleType_CWD' 'SaleType_Con'\n",
      " 'SaleType_ConLD' 'SaleType_ConLI' 'SaleType_ConLw' 'SaleType_New'\n",
      " 'SaleType_Oth' 'SaleType_WD' 'SaleCondition_Abnorml'\n",
      " 'SaleCondition_AdjLand' 'SaleCondition_Alloca' 'SaleCondition_Family'\n",
      " 'SaleCondition_Normal' 'SaleCondition_Partial']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Load your dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X_train = train_df.drop('SalePrice', axis=1)\n",
    "y_train = train_df['SalePrice']\n",
    "\n",
    "# Identify categorical columns (assuming they are of type 'object')\n",
    "categorical_cols = [cname for cname in X_train.columns if \n",
    "                    X_train[cname].dtype == \"object\"]\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Apply the preprocessor to the training data\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Now we can safely perform feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k='all')\n",
    "X_new = selector.fit_transform(X_train_encoded, y_train)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_features_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "# Note: This will be the list of all new feature columns generated by OneHotEncoder\n",
    "selected_features_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "\n",
    "print(\"Selected features:\", selected_features_names[selected_features_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train_processed: 1460\n",
      "Length of y_train: 1460\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Length of X_train_processed:\", X_train_processed.shape[0])\n",
    "print(\"Length of y_train:\", len(y_train))\n",
    "\n",
    "# Ensure they have the same length\n",
    "if X_train_processed.shape[0] == len(y_train):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_processed, y_train, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"The arrays do not have the same number of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'train.csv' is loaded into train_df\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Assuming the preprocessing steps are correctly applied to X_train_processed\n",
    "# Now, align y_train with the processed features\n",
    "y_train_aligned = train_df['SalePrice']\n",
    "\n",
    "# Check if the lengths match now\n",
    "if X_train_processed.shape[0] == len(y_train_aligned):\n",
    "    # Proceed with train-test split\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_processed, y_train_aligned, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"There is still a mismatch in the number of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 26595.627261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the model\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 200)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(xg_reg, X_train_processed, y_train_aligned, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate the mean RMSE across the cross-validated segments\n",
    "mean_rmse = np.mean(np.sqrt(-scores))\n",
    "print(\"Mean RMSE: %f\" % (mean_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 27595.964851\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X_train_processed and y_train_aligned are your preprocessed feature matrix and target vector\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_processed, y_train_aligned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate an XGBoost regressor object with default parameters\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the validation set\n",
    "y_pred = xg_reg.predict(X_valid)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n",
    "print(\"Validation RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation set: 27595.964851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Assuming y_valid are the actual target values and y_pred are the predicted values from your model\n",
    "rmse = sqrt(mean_squared_error(y_valid, y_pred))\n",
    "print(\"RMSE on validation set: %f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE scores: [26499.67835039 34388.28826374 30499.55181851 22308.41424717\n",
      " 31321.51474757]\n",
      "Mean RMSE: 29003.48948547501\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(xg_reg, X_train_processed, y_train_aligned, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Calculate the RMSE for each fold\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "# Output the results\n",
    "print(\"Cross-validated RMSE scores:\", rmse_scores)\n",
    "print(\"Mean RMSE:\", rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = train_data.SalePrice\n",
    "X = train_data.drop(['SalePrice'], axis=1)\n",
    "\n",
    "# Select categorical columns with relatively low cardinality (convenient for one-hot encoding)\n",
    "categorical_cols = [cname for cname in X.columns if\n",
    "                    X[cname].nunique() < 10 and \n",
    "                    X[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X.columns if \n",
    "                  X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Preprocessing of test data, fit model\n",
    "preds_test = my_pipeline.predict(test_data)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': preds_test})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
